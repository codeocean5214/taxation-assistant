# Tax Policy RAG System ğŸ“Š

**LangChain v1.2+ | Llama 3.2 | 100% Free & Local**

A production-ready RAG (Retrieval-Augmented Generation) system for corporate taxation policy Q&A, built with the latest LangChain v1.2 stack.

## âœ¨ Features

- ğŸ†• **LangChain v1.2+**: Latest streamlined API
- ğŸ¤– **Llama 3.2**: Via Ollama (free, local)
- ğŸ“„ **PDF Processing**: Intelligent chunking
- ğŸ” **Semantic Search**: FAISS vector store
- ğŸ’¬ **Chat UI**: Streamlit interface
- ğŸŒ **REST API**: FastAPI backend
- ğŸ”’ **Privacy**: 100% local processing

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Streamlit   â”‚â”€â”€â”€â”€â–¶â”‚              â”‚
â”‚     UI       â”‚     â”‚  RAG Engine  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚  (LangChain  â”‚
                     â”‚     v1.2)    â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚              â”‚
â”‚   FastAPI    â”‚â”€â”€â”€â”€â–¶â”‚              â”‚
â”‚     API      â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â–¼                 â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  FAISS   â”‚      â”‚ChatOllamaâ”‚
            â”‚  Vector  â”‚      â”‚(Llama3.2)â”‚
            â”‚  Store   â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites

- **Python 3.10+** (LangChain v1 requires 3.10+)
- **Ollama** installed and running

### 1. Install Ollama & Llama 3.2

```bash
# Install Ollama
# Windows/Mac: Download from https://ollama.com
# Linux:
curl -fsSL https://ollama.com/install.sh | sh

# Pull Llama 3.2
ollama pull llama3.2

# Start Ollama server (keep running)
ollama serve
```

### 2. Setup Python Environment

```bash
# Clone or navigate to project
cd tax-policy-rag

# Create virtual environment
python -m venv venv

# Activate
venv\Scripts\activate  # Windows
source venv/bin/activate  # Linux/Mac

# Install dependencies
pip install --prefer-binary -r requirements.txt
```

### 3. Process Your PDF

```bash
# Place PDF in data/documents/
# Then build the index:
python scripts/build_index.py data/documents/your_tax_policy.pdf
```

### 4. Run the Application

**Option A: Streamlit UI**
```bash
streamlit run streamlit_app/app.py
```
Open http://localhost:8501

**Option B: FastAPI**
```bash
uvicorn api.main:app --reload
```
API docs at http://localhost:8000/docs

**Option C: CLI**
```bash
python scripts/test_query.py
```

## ğŸ“ Project Structure

```
tax-policy-rag/
â”œâ”€â”€ core/                   # Shared RAG logic
â”‚   â”œâ”€â”€ config.py          # Configuration
â”‚   â”œâ”€â”€ document_processor.py  # PDF processing
â”‚   â”œâ”€â”€ vector_store.py    # FAISS embeddings
â”‚   â””â”€â”€ rag_engine.py      # Query processing
â”œâ”€â”€ api/
â”‚   â””â”€â”€ main.py            # FastAPI endpoints
â”œâ”€â”€ streamlit_app/
â”‚   â””â”€â”€ app.py             # Streamlit UI
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ build_index.py     # Index builder
â”‚   â””â”€â”€ test_query.py      # CLI tester
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ documents/         # PDFs
â”‚   â””â”€â”€ vector_db/         # FAISS index
â””â”€â”€ requirements.txt
```

## ğŸ”§ Configuration

Edit `core/config.py`:

```python
# Model selection
OLLAMA_MODEL = "llama3.2"  # or "mistral", "phi3", etc.

# Chunking strategy
CHUNK_SIZE = 1000
CHUNK_OVERLAP = 200

# Retrieval
TOP_K_RETRIEVAL = 4  # Chunks per query
```

## ğŸ’¡ Usage Examples

### Streamlit
1. Upload PDF via sidebar
2. Click "Process PDF"
3. Ask questions in chat

### API

**Single Query:**
```bash
curl -X POST http://localhost:8000/query \
  -H "Content-Type: application/json" \
  -d '{
    "question": "What is the corporate tax rate?",
    "return_sources": true,
    "k": 4
  }'
```

**Python:**
```python
import requests

response = requests.post(
    "http://localhost:8000/query",
    json={"question": "Explain R&D tax credits"}
)
print(response.json()["answer"])
```

## ğŸ†• What's New in LangChain v1.2

This project uses LangChain v1.2+ features:

- âœ… **ChatOllama**: Official Ollama integration via `langchain-ollama`
- âœ… **LCEL**: LangChain Expression Language for chains
- âœ… **Streamlined imports**: Clean v1 namespace
- âœ… **Better type hints**: Full typing support
- âœ… **Improved docs**: Unified documentation

### Migration from v0.x

Key changes:
```python
# Old (LangChain 0.x)
from langchain.llms import Ollama
from langchain.chains import RetrievalQA

# New (LangChain v1.2+)
from langchain_ollama import ChatOllama
from langchain_core.runnables import RunnablePassthrough
```

## ğŸ› Troubleshooting

**Ollama not found:**
```bash
# Check if Ollama is running
curl http://localhost:11434/api/tags

# If not, start it:
ollama serve
```

**Model not found:**
```bash
# List installed models
ollama list

# Install llama3.2
ollama pull llama3.2
```

**Import errors:**
```bash
# Clean reinstall
pip uninstall langchain langchain-community langchain-core langchain-ollama -y
pip install --prefer-binary -r requirements.txt
```

**Slow responses:**
- First query loads model (~5-10 sec)
- Subsequent queries: 3-5 sec
- Use smaller model for faster: `ollama pull phi3`

## ğŸ“š Resources

- [LangChain v1 Docs](https://docs.langchain.com)
- [Ollama Models](https://ollama.com/library)
- [FAISS Documentation](https://github.com/facebookresearch/faiss/wiki)
- [LangChain v1 Migration Guide](https://docs.langchain.com/oss/python/releases/langchain-v1)

## ğŸ¯ Performance

- **First query**: 10-15 sec (model loading)
- **Subsequent**: 3-5 sec
- **RAM**: 4-6GB (for Llama 3.2)
- **Disk**: 2-3GB (model + vectors)

## ğŸ¤ Contributing

This is a learning project demonstrating LangChain v1.2 best practices. Feel free to:
- Experiment with different models
- Improve prompts
- Add evaluation metrics
- Build multi-document support

## ğŸ“„ License

MIT License

## ğŸ™ Acknowledgments

- **LangChain**: Modern LLM framework
- **Ollama**: Local LLM deployment
- **Meta**: Llama 3.2 model
- **FAISS**: Fast similarity search

---

**Built with LangChain v1.2+ | January 2025**
